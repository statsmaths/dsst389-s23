---
title: "Notebook 08 -- Solutions"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. You may also have to hit the broom in
the upper right-hand corner of the window. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

I have set the options `message=FALSE` and `echo=FALSE` to avoid cluttering
your solutions with all the output from this code.

## Reading the Data

Today we are going to look at a subset of a well-known text analysis corpus
call NewsGroups-20. It's an old set of mailing list archives from 20 different
categories.

```{r, message = FALSE}
docs <- read_csv("../data/newsgroups.csv.bz2")
anno <- read_csv("../data/newsgroups_token.csv.bz2")
```

## Questions

### Supervised Learning

Build an elastic net model to determine the category of the newsgroup messages:

```{r, question-01}
model <- dsst_enet_build(anno, docs)
```

Produce a confusion matrix for the messages using just the validation data.
Take note of any commonly confused categories:

```{r, question-02}
model$docs %>%
  filter(train_id == "valid") %>%
  select(label, pred_label) %>%
  table()
```

Look at the coefficients from the model; perhaps use `lambda_num = 30`. Use
the code from Project 2 to look at the positive (and negative, if there are any)
terms associated with each category. Do the terms seem to correspond to the
categories in an expected way? Note: You can pipe the whole thing into the
function `View()` if you want a better way to look at the output in RStudio.

```{r, question-03}
dsst_coef(model$model, lambda_num = 30, to_tibble = TRUE) %>%
  filter(term != "(Intercept)") %>%
  pivot_longer(names_to = "label", values_to = "coef", cols = -c(term, MLN)) %>%
  filter(coef != 0) %>%
  mutate(direction = if_else(sign(coef) > 0, "positive", "negative")) %>%
  group_by(label, direction) %>%
  summarize(term = paste(term, collapse = " | ")) %>%
  pivot_wider(
    id_cols = "label",
    values_from = "term",
    names_from = "direction",
    values_fill = ""
  ) #%>%
  #View()
```

Now, use the G-score metrics to find the 4 terms that are most associated with
each category. Again, do these seem to match your intuition?

```{r, question-04}
dsst_metrics(anno, docs) %>%
  group_by(label) %>%
  slice_head(n = 4)
```

### Unsupervised Learning

Let's move on to new material. Compute the first two principal components of
the categories. Remember to set the document variable to "label".

```{r, question-05}
anno %>%
  inner_join(docs, by = "doc_id") %>%
  dsst_pca(doc_var = "label")
```

Plot (in R) the first two principal components of the categories. Add labels
using a text repel layer. Try to find some of the document pairs in the PCA
plot.

```{r, question-06}
anno %>%
  inner_join(docs, by = "doc_id") %>%
  dsst_pca(doc_var = "label") %>%
  ggplot(aes(v1, v2)) +
    geom_point() +
    geom_text_repel(aes(label = label))
```

Now, produce a corresponding UMAP plot. Is this easier or more difficult to
interpret?

```{r, question-07}
anno %>%
  inner_join(docs, by = "doc_id") %>%
  dsst_umap(doc_var = "label") %>%
  ggplot(aes(v1, v2)) +
    geom_point() +
    geom_text_repel(aes(label = label))
```

Next, produce the principal components for the messages themselves. Save the
results as a JSON file and go to the link below to visualize the results. Color
the points based on the labels.

- https://statsmaths.github.io/text-embed-explo/build/

```{r, question-08}
dsst_pca(anno) %>% dsst_json_drep(docs, color_var = "label")
```

Repeat the last question for the UMAP parameters. Did you find any interesting
clusters of documents?

```{r, question-09}
dsst_umap(anno) %>% dsst_json_drep(docs, color_var = "label")
```

Make sure to not rush through this step; take a couple minutes to pan around
in the embedding space.
