---
title: "Notebook 05 -- Solutions"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. You may also have to hit the broom in
the upper right-hand corner of the window. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

I have set the options `message=FALSE` and `echo=FALSE` to avoid cluttering
your solutions with all the output from this code.

## Reading the Data

Today we are going to look at a dataset of short texts taken from a set of 3
American authors:

```{r, message = FALSE}
docs <- read_csv("../data/stylo_us.csv")
anno <- read_csv("../data/stylo_us_token.csv.gz")
```

The prediction task is to determine the identity of the author based on the
text.

## Questions

### Authorship Detection with the Elastic Net

To start, build an elastic net model using the default term-frequency features
to predict the authorship of each text. 

```{r, question-01}
model <- dsst_enet_build(anno, docs)
```

Now, compute the overall error rate of this model for each of the training and 
validations sets. You should see that the model is much better on the training
set than it is on the validation set.

```{r, question-02}
model$docs %>%
  group_by(train_id) %>%
  summarize(erate = mean(label != pred_label))
```

In the code block below, look at the model coefficients. You may want to limit
the number of results by setting `lambda_num` to something around 25.

```{r, question-03}
dsst_coef(model$model, lambda_num = 25)
```

You should notice several interesting things about the model. What are the main
features that are being used here? Do you see any patterns about where the
strongest coefficients are concentrated?

Build another elastic net model but remove the proper nouns from the texts.

```{r, question-04}
model <- anno %>%
  filter(upos != "PROPN") %>%
  dsst_enet_build(docs)
```

What is the error rate of the model now? Note how it changes from above.

```{r, question-05}
model$docs %>%
  group_by(train_id) %>%
  summarize(erate = mean(label != pred_label))
```

In the code block below, look at the model coefficients. You may want to limit
the number of results by setting `lambda_num` to something around 30.

```{r, question-06}
dsst_coef(model$model, lambda_num = 30)
```

You should now notice that the types of words being used are very different.
What are the primary qualities of (most of) the terms now being selected?

Now, build an elastic net model using the "xpos" column to build frequencies
(i.e., set "xpos" to the `token_var` parameter). These codes offer a more
granular way of describing parts of speech.

```{r, question-07}
model <- anno %>%
  dsst_enet_build(docs, token_var = "xpos")
```

Compute the error rate, comparing it to the previous error rates.

```{r, question-08}
model$docs %>%
  group_by(train_id) %>%
  summarize(erate = mean(label != pred_label))
```

We can do a bit better using n-grams of xpos tags. Create a model using 3-grams
of xpos tags.

```{r, question-09}
model <- anno %>%
  dsst_ngram(token_var = "xpos", n = 3, n_min = 1) %>%
  dsst_enet_build(docs)
```

Finally, compute the error rate of this model:

```{r, question-10}
model$docs %>%
  group_by(train_id) %>%
  summarize(erate = mean(label != pred_label))
```

How does it compare the word-based model above? It will likely be very close,
though perhaps still slightly worse.

### Authorship Detection with the k-nearest neighbors

Now, let's use the k-nearest neighbors method to build a model. Start by
creating a knn model with all of the default features and k = 1 (the default).

```{r, question-11}
model <- anno %>%
  dsst_knn_build(docs)
```

Now, print out the error rate of the model. Compare to the same model using
the elastic net.

```{r, question-12}
model$docs %>%
  group_by(train_id) %>%
  summarize(erate = mean(label != pred_label))
```

You should have noticed that knn is significantly worse than the elastic net
model. Take a few moments and think about what the difference between these
models is and why knn is not particularly suitable to the prediction task here.

### Data Visualization

For the last few questions, we'll do some data visualization directly using the
information in the `anno` and `docs` tables. The general pattern of how to do
these is to first manipulate and summarize the annotations, then join to the
documents table to get the labels, and finally summarize again and plot the
results.

For the first question, create a bar plot (remember, this needs `geom_col`) 
that shows the average length of sentences in the data broken down by the 
author and whether the data comes from the training or validation set. Use
the fill color to distinguish the train/valid split (hint: you'll need to set
position = "dodge" in the geometry).

```{r, question-13}
anno %>%
  group_by(doc_id, sid) %>%
  summarize(n = n()) %>%
  left_join(docs, by = "doc_id") %>%
  group_by(train_id, label) %>%
  summarize(avg = mean(n)) %>%
  ggplot(aes(label, avg)) +
    geom_col(aes(fill = train_id), position = "dodge")
```

Now, for each combination of the train/valid set and author, compute the average
number of verbs in each sentence and the average number of adjectives in each
sentence. Plot these two measurements using a scatter plot, with color
indicating the author and shape indicating the train/valid split.

```{r, question-14}
anno %>%
  group_by(doc_id, sid) %>%
  summarize(n_verb = sum(upos == "VERB"), n_adj = sum(upos == "ADJ")) %>%
  left_join(docs, by = "doc_id") %>%
  group_by(train_id, label) %>%
  summarize(
    avg_verb = mean(n_verb),
    avg_adj = mean(n_adj)
  ) %>%
  ggplot(aes(avg_verb, avg_adj)) +
    geom_point(aes(color = label, shape = train_id), size = 3)
```

Now, we will create a plot that shows the average number of verbs in a sentence
for each combination of the author and train/valid set. In this plot, put the
author on the x-axis and the average on the y-axis; use color to distinguish
the train/valid split. In addition, compute what we call the *standard error*
of the average number of verbs by dividing the standard deviation (the function
`sd`) of the number of verbs by the square-root (`sqrt`) of the number of the
number of sentences. Finally, plot the data using the geometry `geom_pointrange`;
this requires setting the x-aesthetic, y-aesthetic, as well as ymin and ymax
aesthetic. Set the values of these last two to be the average number of verbs
minus and plus the standard error time two. 

This sounds like a lot of work, but the code is actually not very long. What it
does is produce confidence intervals for a measurement of where we would expect
the average values to be if we re-sampled in a similar way from the same data.

```{r, question-15}
anno %>%
  group_by(doc_id, sid) %>%
  summarize(n_verb = sum(upos == "VERB")) %>%
  left_join(docs, by = "doc_id") %>%
  group_by(train_id, label) %>%
  summarize(avg = mean(n_verb), s = sd(n_verb) / sqrt(n())) %>%
  ggplot(aes(label, avg)) +
    geom_pointrange(aes(
      ymin = avg - 2 * s,  ymax = avg + 2 * s, color = train_id
    ))
```

This is a tricky question if you've never worked with confidence intervals 
before. Just take a look at the solutions and try to understand how to interpret
the results. We will talk more about this model and others like it next week.

A final note: in all of these visualizations, I had you split the training and
validations datasets and look at them in relation to one another. This is 
because of the special nature of the train/valid split (they are different 
novels) in this example. In the project, I'd recommend replicating plots like
these using only the validation data.
