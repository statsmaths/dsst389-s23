---
title: "Notebook 15 -- Solutions"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---


```{r, include=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

## Parsing Wikipedia

In previous notes we have used the MediaWiki API to grab information about the
number of page views there have been in the past 60 days for each page
associated with a row in our `foods` dataset. The API also allows us to grab
data about the actual current content of any page on Wikipedia. Here is the
query string for accessing the page for "apple":

```{r}
url_str <- modify_url("https://en.wikipedia.org/w/api.php",
                       query = list(action = "parse", format = "json", page = "apple"))

res <- dsst_cache_get(url_str, cache_dir = "cache")
obj <- content(res, type = "application/json")
```

Calling the query string yields a JSON object that contains some metadata about
the page contents. It also contains a text representation of the page parsed
into HTML. In summary, the JSON file contains an element that is in HTML (!).
Here is the a sample of the HTML file:

```{r}
html_txt <- obj$parse$text[["*"]]
stri_wrap(stri_sub(html_txt, 1, 1000), 70)
```

Now, we can use the `read_html` function to convert the text into an
`html_document`.

```{r}
obj_html <- read_html(html_txt)
obj_html
```

The larger JSON object contains a set of all links that are found on the page,
but it does not tell us where they occur in the page. This can be a problem
because it includes lots of extra links found at the top and bottom on the page.
To get a more accurate sense of the links within the body of the text, we need
to grab them from the parsed HTML.

In the code below, extract all of the links (the "a" tag) that are somewhere 
inside the paragraph tags ("p") of the Wikipedia text.

```{r, question-01}
xml_find_all(obj_html, ".//p//a")
```

Now, create a data table that has three columns: `item` (which is always equal to
"Apple"), `link` giving the href url within each of the links found above, and
`text` giving the text inside the links above. Name the dataset `wiki_links`

```{r, question-02}
wiki_links <- tibble(
  item = "Apple",
  link = xml_attr(xml_find_all(obj_html, ".//p//a"), "href"),
  text = xml_text(xml_find_all(obj_html, ".//p//a"))
)

wiki_links
```

Some of the links you found above may be to external pages or to footnotes. 
Filter the `wiki_links` to include only those that start with "/wiki/" and 
then remove the leading "/wiki/" from the remaining links. Resave the data 
as `wiki_links`.

```{r, question-03}
wiki_links <- wiki_links %>%
  filter(stri_sub(link, 1L, 6L) == "/wiki/") %>%
  mutate(link = stri_sub(link, 7L, -1L))

wiki_links
```

Now, without saving the output, look at the items in `wiki_links` where a
lowercase version of the link does not each a lowercase version of the text.
What differences do you see?

```{r, question-04}
wiki_links %>%
  filter(stri_trans_tolower(link) != stri_trans_tolower(text))
```

Now, create a summarized version of `wiki_links` that has one row for each
unique link, with a count column. Arrange the links from the highest to the
lowest number. You don't need the text column here.

```{r, question-05}
wiki_links %>%
  group_by(item, link) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  arrange(desc(n))
```

## Covid Dashboard

As a second task, let's try to use our XML skills to grab data from the UR 
COVID dashboard. I will give you the code here to grab the page; you may find
it useful to visit the page in a browser as well.

```{r}
url_str <- modify_url("https://www.richmond.edu/coronavirus/dashboard/index.html",)
res <- dsst_cache_get(url_str, cache_dir = "cache")
obj <- content(res, type = "text/html", encoding = "UTF-8")
obj
```

The number of cases from the past seven weeks is given in an html table, which
we can find with the `tbody` tag. Extract this table (there is only one) in
the code below:

```{r}
xml_find_all(obj, ".//tbody")
```

Inside the table are table rows (`tr`) and inside each table row of a table 
header (`th`). Grab the text of these seven table headers below:

```{r}
xml_text(xml_find_all(obj, ".//tbody/tr/th"))
```

Each table row also has a table data element (`td`); usually there are more than
one of these in each row, but here there's just one. In this example, there is
also a paragraph tag (`p`) inside each table data element. Grab the text of
these paragraphs below:

```{r}
xml_text(xml_find_all(obj, ".//tbody/tr/td/p"))
```

Putting together your last two answers, create an tibble that has the date of
each record and count of positive COVID-19 cases. Save the data as an object
named `cases`.

```{r}
cases <- tibble(
  date = xml_text(xml_find_all(obj, ".//tbody/tr/th")),
  count = as.numeric(xml_text(xml_find_all(obj, ".//tbody/tr/td/p")))
)
cases
```

Finally, recreate the bar plot from the website in R using your data from above.

```{r}
cases %>%
  mutate(date = fct_inorder(date)) %>%
  ggplot(aes(date, count)) +
    geom_col()
```



