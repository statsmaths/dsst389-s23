---
title: "Notebook 11 -- Solutions"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. You may also have to hit the broom in
the upper right-hand corner of the window. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

I have set the options `message=FALSE` and `echo=FALSE` to avoid cluttering
your solutions with all the output from this code.

## Associated Press 

We are going to look at two different data sets today. The first is a set of
news articles from the associated press. There is no predictive task. Our only
goal is to apply unsupervised techniques to the data to understand the structure
and themes of the collection. Start by loading the data:

```{r, message = FALSE}
docs <- read_csv(file.path("..", "data", "ap.csv.bz2"))
anno <- read_csv(file.path("..", "data", "ap_tokens.csv.bz2"))
```

Now, create a dataset (give it a name, you'll need it later) that creates the
first two principal components based on the nouns and verbs in the data. Set
the `min_df` to be zero to avoid errors. Finally, add kmeans clustering with 20
clusters, and add a column called `train_id` which is always equal to "train".

```{r, question-01}
dt <- anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_pca(min_df = 0) %>%
  dsst_kmeans(n_clusters = 20) %>%
  mutate(train_id = "train")

dt
```

Next, compute the size of each cluster. Notice that these are not equally sized,
but the variation should be within an order of magnitude. 

```{r, question-02}
dt %>%
  group_by(cluster) %>%
  summarize(n = n())
```

Using the `dsst_metrics` function, compute the nouns and verbs most associated
with each cluster. Print out the top 10 words for each cluster. Take a few
moments to look at the results and try to make sense of them.

```{r, question-03}
anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_metrics(dt, label_var = "cluster") %>%
  filter(count > expected) %>%
  group_by(label) %>%
  slice_head(n = 10) %>%
  summarize(words = paste0(token, collapse = " | ")) %>%
  getElement('words')
```

Next, use the function `slice_sample` with `n = 1` to randomly select one
article from each cluster. Explore a few examples and see how they line up with
the words above.

```{r, question-04}
docs %>%
  inner_join(dt, by = c("doc_id", "train_id")) %>%
  group_by(cluster) %>%
  slice_sample(n = 1) %>%
  dsst_print_text()
```

Now, fit a topic model with the nouns and verbs from the data with 20 topics.
As above, set `min_df = 0`.

```{r, question-05}
model <- anno %>%
  filter(upos %in% c("NOUN", "VERBS")) %>%
  dsst_lda_build(num_topics = 20, min_df = 0)
```

Modifying the code from the notes, compute the 10 words most associated with
each topic. Do you see similar patterns fro the PCA + G-Score analysis? What
are some notable differences? Do any topics line up well with clusters?

```{r, question-06}
model$terms %>%
  group_by(topic) %>%
  arrange(desc(beta)) %>%
  slice_head(n = 10) %>%
  summarise(words = paste(token, collapse = "; "))
```

Export your topic model to a JSON file.

```{r, question-07}
dsst_json_lda(model, docs)
```

Load the file into the web interface and explore the data. Think about the
kinds of things you can and cannot learn by this method and how it compares 
to the clustering analysis.

## Nematode Abstracts

As a second task, we will look at a collection of abstract from the study of
Nematodes. Read the data in with the following:

```{r, message = FALSE, warning = FALSE}
docs <- read_csv(file.path("..", "data", "nematode_abs.csv.bz2"))
anno <- read_csv(file.path("..", "data", "nematode_abs_tokens.csv.bz2"))
```

The questions below here use the same code as the section above. It's the
output that  will be different.

Now, create a dataset (give it a name, you'll need it later) that creates the
first two principal components based on the nouns and verbs in the data. Set
the `min_df` to be zero to avoid errors. Finally, add kmeans clustering with 20
clusters, and add a column called `train_id` which is always equal to "train".

```{r, question-08}
dt <- anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_pca(min_df = 0) %>%
  dsst_kmeans(n_clusters = 20) %>%
  mutate(train_id = "train")

dt
```

Next, compute the size of each cluster. Notice that these are not equally sized,
but the variation should be within an order of magnitude. 

```{r, question-09}
dt %>%
  group_by(cluster) %>%
  summarize(n = n())
```

Using the `dsst_metrics` function, compute the nouns and verbs most associated
with each cluster. Print out the top 10 words for each cluster. Take a few
moments to look at the results and try to make sense of them.

```{r, question-10}
anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_metrics(dt, label_var = "cluster") %>%
  filter(count > expected) %>%
  group_by(label) %>%
  slice_head(n = 10) %>%
  summarize(words = paste0(token, collapse = " | ")) %>%
  getElement('words')
```

Next, use the function `slice_sample` with `n = 1` to randomly select one
article from each cluster. Explore a few examples and see how they line up with
the words above.

```{r, question-11}
docs %>%
  inner_join(dt, by = c("doc_id", "train_id")) %>%
  group_by(cluster) %>%
  slice_sample(n = 1) %>%
  dsst_print_text()
```

Now, fit a topic model with the nouns and verbs from the data with 20 topics.
As above, set `min_df = 0`.

```{r, question-12}
model <- anno %>%
  filter(upos %in% c("NOUN", "VERBS")) %>%
  dsst_lda_build(num_topics = 20, min_df = 0)
```

Modifying the code from the notes, compute the 10 words most associated with
each topic. Do you see similar patterns fro the PCA + G-Score analysis? What
are some notable differences? Do any topics line up well with clusters?

```{r, question-13}
model$terms %>%
  group_by(topic) %>%
  arrange(desc(beta)) %>%
  slice_head(n = 10) %>%
  summarise(words = paste(token, collapse = "; "))
```

Export your topic model to a JSON file.

```{r, question-14}
dsst_json_lda(model, docs)
```

Load the file into the web interface and explore the data. Think about the
kinds of things you can and cannot learn by this method and how it compares 
to the clustering analysis.

## Further Exploration

If you have remaining time, return to the Associated Press articles data. Use
UMAP in place of PCA and take a much larger (50? 100?) number of clusters. How
do the clusters compare in the previous results? Then, do a topic model with
50 topics. Do these provide a better of worse understanding of the data? 

