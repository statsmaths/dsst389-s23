---
title: "Notebook 03"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. You may also have to hit the broom in
the upper right-hand corner of the window. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

I have set the options `message=FALSE` and `echo=FALSE` to avoid cluttering
your solutions with all the output from this code.

## Reading the Data

To start, let's read in some text data to study for today. The dataset for this
notebook consists of some spam text messages.

```{r, message = FALSE}
docs <- read_csv("../data/spam.csv")
anno <- read_csv("../data/spam_token.csv.gz")
```

The data is a bit old (I think around 2005) and from the UK. It is fairly easy
to classify this dataset, so I think it is a good place to start.

## Questions

In the code blocks below, you should type in the solution to each of the
questions. Take your time with this; the code is not complex. The real point is
to understand each of the steps and what we learn about the data from the model.
There are also some short answers that you can fill in after the phrase "Answer".

Start by creating an object called `model` that builds an elastic net to
predict whether a message is spam or not. Use all of the default values.

```{r, question-01}
model <- dsst_enet_build(anno, docs)
```

Now, compute the error rate for the training and validation sets.

```{r, question-02}
dsst_erate(model)
```

Does the model do better on the training or the validation data?
**Answer:** Better on the training data.

In the next code block, compute the segmented error rate.

```{r, question-03}
dsst_erate(model, segmented = TRUE)
```

And next, compute the confusion matrix:

```{r, question-04}
dsst_confusion_matrix(model)
```

Is the model more likely to think that spam is ham, or ham is spam?
**Answer:** My model was more likely to think that spam was ham.

In the next step, compute the coefficients of the model. Note that the model
has fit a multinomial elastic net, but with only two classes. You should see
that there are two columns with opposite signs. See question 4 on the first
handout for an explanation of why this happens.

```{r, question-05}
dsst_coef(model)
```

Look at the first few values in the table above. Do the strongest features that
the model found make sense to you as an indicator or spam?
**Answer:** I found the strongest features to be "call", "Â£", "to" and "!". All
the signs are positive for spam. I can certainly imagine all of these featuring
in a spam text message.

Reproduce the coefficients, cutting off after a particular lambda number to get
only a small number of results (about a dozen is good).

```{r, question-06}
dsst_coef(model, lambda_num = 27)
```

Next, print out 10 messages that have the highest probability of being spam.
Use the function `dsst_print_text()` to print out the messages and take a moment
to read them.

```{r, question-07}
dsst_max_prob(model, n = 10, real_label = "spam") %>% dsst_print_text()
```

Repeat the last question, but select the ten messages most likely to be ham.

```{r, question-08}
dsst_max_prob(model, n = 10, real_label = "ham") %>% dsst_print_text()
```

Next, print out ten spam messages that were mis-classified.

```{r, question-09}
dsst_neg_examples(model, n = 10, real_label = "spam") %>% dsst_print_text()
```

Now, print out ten ham messages that were mis-classified.

```{r, question-10}
dsst_neg_examples(model, n = 10, real_label = "ham") %>% dsst_print_text()
```

In the next code block, select one of the coefficients from the model and use
the keywords in context function to find examples of the lemma in the texts.
If there is one, try to select a term that you had trouble understanding why it
was significant. See if the KWiC method helps explain the significance of the
term.

```{r, question-11}
dsst_kwic("150p", anno)
```

Notice that the KWiC output gives us the document ids for the messages. If we
want to learn more we can use the function `filter` to select the entire
message. Below is an example for the message "doc00164". Replace the id with
one from your result above and read the entire text message.

```{r, question-12}
filter(docs, doc_id == "doc00482")$text
```

For a final step, take a moment to really think deeply about all of the results.
Try to explain in a few sentences everything you've learned about the data.
**Answer:** Here's my takeaway: We can identify the spam messages with about
a 7% error rate by identifying a few common terms that are associated with spam
and ham. Spam terms are related to getting people to respond and often are
related to money; there are fewer ham terms and these tend to be common function
words and punctuation marks. Most of the model mistakes included messages with
lots of misspellings as well as a few ham messages that seem to be mislabelled.
