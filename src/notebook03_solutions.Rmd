---
title: "Notebook 03 -- Solutions"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. You may also have to hit the broom in
the upper right-hand corner of the window. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

I have set the options `message=FALSE` and `echo=FALSE` to avoid cluttering
your solutions with all the output from this code.

## Reading the Data

To start, let's read in some text data to study for today. The dataset for this
notebook consists of some spam text messages.

```{r, message = FALSE}
docs <- read_csv("../data/spam.csv")
anno <- read_csv("../data/spam_token.csv.gz")
```

The data is a bit old (I think around 2005) and from the UK. It is fairly easy
to classify this dataset, so I think it is a good place to start.

## Questions

In the code blocks below, you should type in the solution to each of the
questions. Take your time with this; the code is not complex. The real point is
to understand each of the steps and what we learn about the data from the model.
There are also some short answers that you can fill in after the phrase "Answer".

Start by creating an object called `model` that builds an elastic net to
predict whether a message is spam or not. Use all of the default values.

```{r, question-01}
model <- dsst_enet_build(anno, docs)
```

Now, compute the error rate for the training and validation sets.

```{r, question-02}
model$docs %>%
  group_by(train_id) %>%
  summarize(erate = mean(label != pred_label))
```

Does the model do better on the training or the validation data?
**Answer:** Better on the training data.

In the next code block, compute the segmented error rate.

```{r, question-03}
model$docs %>%
  group_by(train_id, label) %>%
  summarize(erate = mean(label != pred_label))
```

And next, compute the confusion matrix:

```{r, question-04}
model$docs %>%
  select(label, pred_label, train_id) %>%
  table()
```

Is the model more likely to think that spam is ham, or ham is spam?
**Answer:** My model was more likely to think that spam was ham.

In the next step, compute the coefficients of the model. Note that the model
has fit a multinomial elastic net, but with only two classes. You should see
that there are two columns with opposite signs. See question 4 on the first
handout for an explanation of why this happens.

```{r, question-05}
dsst_coef(model$model)
```

Look at the first few values in the table above. Do the strongest features that
the model found make sense to you as an indicator or spam?
**Answer:** I found the strongest features to be "call", "Â£", "to" and "!". All
the signs are positive for spam. I can certainly imagine all of these featuring
in a spam text message.

Reproduce the coefficients, cutting off after a particular lambda number to get
only a small number of results (about a dozen is good).

```{r, question-06}
dsst_coef(model$model, lambda_num = 27)
```

Next, print out 10 messages that have the highest probability of being spam.
Use the function `dsst_print_text()` to print out the messages and take a moment
to read them.

```{r, question-07}
model$docs %>%
  filter(pred_label == "spam") %>%
  arrange(desc(pred_value))  %>%
  slice_head(n = 10) %>%
  dsst_print_text()
```

Repeat the last question, but select the ten messages most likely to be ham.

```{r, question-08}
model$docs %>%
  filter(pred_label == "ham") %>%
  arrange(desc(pred_value))  %>%
  slice_head(n = 10) %>%
  dsst_print_text()
```

Next, print out ten random spam messages that were mis-classified.

```{r, question-09}
model$docs %>%
  filter(label == "spam") %>%
  filter(label != pred_label) %>%
  slice_sample(n = 10) %>%
  dsst_print_text()
```

Now, print out ten random ham messages that were mis-classified.

```{r, question-10}
model$docs %>%
  filter(label == "ham") %>%
  filter(label != pred_label) %>%
  slice_sample(n = 10) %>%
  dsst_print_text()
```

Try to reproduce the model you created in question 1 and produce another
confusion matrix on the validation dataset. This time, however, do not
use any of the `dsst_*` functions. Try to only include lines from the notes
that are needed, avoiding those code lines that I only showed to illustrate
the intermediate steps.

```{r, question-11}
# Find the set of documents and lemmas
document_set <- unique(anno$doc_id)
vocab_set <- unique(anno$lemma)

# Match documents and lemmas to the sets above
dindex <- as.numeric(match(anno$doc_id, document_set))
tindex <- as.numeric(match(anno$lemma, vocab_set))

# Create a model matrix
X <- Matrix::sparseMatrix(
  i = dindex, j = tindex, x = 1,
  dims = c(length(document_set), length(vocab_set)),
  dimnames = list(document_set, vocab_set)
)

# Filter model matrix and create train/validition sets
X <- X[, colSums(X) >= 20]
X_train <- X[docs$train_id == "train",]
X_valid <- X[docs$train_id == "valid",]
y_train <- docs$label[docs$train_id == "train"]
y_valid <- docs$label[docs$train_id == "valid"]

# Fit the cross-validated multinomial elastic net on the data; you
# could do a logistic family instead, but it's easier to just always
# do multinomial
model <- glmnet::cv.glmnet(
  X_train, y_train, family = "multinomial", lambda_min_ratio = 0.1, nfolds = 3, 
)

# Predict the output and look at the confusion matrix
y_valid_pred <- predict(model, newx = X_valid, type = "class")
table(y_valid, y_valid_pred)
```

