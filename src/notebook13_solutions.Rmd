---
title: "Notebook 13 -- Solutions"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.


```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

Today's questions are more open-ended than in some of the previous notebooks.

### Time API

Let's come back to the largest cities data. This data contains a code for the
latitude and longitude of each of the 81 cities:

```{r}
cities <- read_csv("../data/largest_cities.csv")
cities
```

The time API we saw today has an API endpoint for looking up the time zone of
a particular longitude and latitude. The URL has the following elements:

  protocol:            https
  authority:           www.timeapi.io
  path:                /api/Time/current/coordinate 
  query parameters:    longitude and latitude
  
Use this information to build a data set with one row for each city in `cities`
and six columns: the city name, the longitude, the latitude, the name of the
timezone, the current datetime, and the current hour.

```{r, question-01}
url_base <- modify_url("https://www.timeapi.io/api/Time/current/coordinate")

n <- nrow(cities)
output <- vector("list", length = n)

for (i in seq_len(n))
{
  url_str <- modify_url(url_base, query = list(
    longitude = cities$lon[i], latitude = cities$lat[i]
  ))
  res <- dsst_cache_get(url_str, cache_dir = "cache")
  
  obj <- content(res, type = "application/json")
  output[[i]] <- tibble(
    name = cities$name[i],
    lon = cities$lon[i],
    lat = cities$lat[i],
    timezone = obj$timeZone,
    datetime = ymd_hms(obj$dateTime),
    hour = obj$hour
  )
}

output <- bind_rows(output)
output
```

Now, use the data to show a visualization of the current hours of the day in 
different world cities:

```{r, question-02}
output %>%
  mutate(hour = factor(hour)) %>%
  ggplot(aes(lon, lat)) +
    geom_point(aes(color = hour), size = 3)
```

## Aesops Fables

Similar to the XKCD API, we can grab the text of Aesops fables from read.gov
through the following URL for any id between 2 and 147.

```{r}
i <- 10
url_str <- modify_url(sprintf("https://www.read.gov/aesop/%03d.html", i))
```

This is HTML format. We can parse it just as we did the CNN Lite data to get
the text in paragraph codes (p) and the title in the header tag h1.

In the following code, create a docs that contains the title and text of each
fable. Please ask for help; you'll probably need it!

```{r, question-03}
docs <- vector("list", length = 146)

for (i in seq(1, 146))
{
  url_str <- modify_url(sprintf("https://www.read.gov/aesop/%03d.html", i + 1))
  res <- dsst_cache_get(url_str, cache_dir = "cache")
  obj <- content(res, type = "text/html", encoding = "UTF-8")
  
  docs[[i]] <- tibble(
    doc_id = xml_text(xml_find_all(obj, "..//h1")),
    train_id = "train",
    text = paste0(xml_text(xml_find_all(obj, "..//p")), collapse = " ")
  )
}

docs <- bind_rows(docs)
docs
```

Two of the fable names are repeated. I suggest removing them:

```{r}
docs <- docs %>%
  filter(!duplicated(doc_id))
```

Now, create an annotation table of the fables:

```{r, question-04}
library(cleanNLP)
cnlp_init_udpipe("english")

docs <- filter(docs, stringi::stri_length(text) > 0)
anno <- cnlp_annotate(docs)$token
```

Now, use the data to produce a UMAP plot of the nouns and verbs along with the
titles of the stories. Make the font size small (2?) to allow the plot to be
more readable.

```{r, question-05}
anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_umap() %>%
  ggplot(aes(v1, v2)) +
    geom_point() +
    geom_text_repel(aes(label = doc_id), size = 2)
```

Finally, print out the top five words according to the G-scores for each fable.
Can you tell/remember the moral based on the words?

```{r, question-06}
anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_metrics(docs, label_var = "doc_id") %>%
  filter(count > expected) %>%
  group_by(label) %>%
  slice_head(n = 6L) %>%
  summarize(terms = paste0(token, collapse = "; ")) %>%
  mutate(out = sprintf("%30 s => %s", stri_sub(label, 1, 30), terms)) %>%
  getElement("out") %>%
  cat(sep = "\n")
```
