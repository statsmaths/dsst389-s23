---
title: "16. Building a Corpus"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
options(dsst.traceit = FALSE)
```

## Creating a Corpus of Wikipedia Documents

For Project 4, unlike the other projects, you will be constructing your own
corpus of documents to work with. These documents will come from the text of
Wikipedia articles. You will build a corpus of documents by starting with one
or more pages that have thematic lists of other pages, and then creating the
corpus by selecting the list in question. This project is important in two
ways.First, you will see how to build and annotate a corpus from scratch.
Secondly, this data is of a different format than others that we have
seen. It has a much shorter set of longer documents, more similar to what
happens when we selected a different `doc_id` in the unsupervised learning
tasks.

## An Example: List of French Philosophers

As usual, I have wrapped up a lot of the complex R code for you in a few
functions. Scraping and organising data from Wikipedia is something we do in
Math 289 (Intro. Data Science), so if you want to learn more of the details,
consider taking that class.

To start, we need to grab a single Wikipedia page that contains the links that
we want to build a corpus from. I will use the page
"List_of_French_philosophers", which has a list of notable French-language
philosophers. I will use the function `dsst_wiki_load` to download this page
from the official Wikipedia API:

```{r}
obj <- dsst_wiki_load("List_of_French_philosophers")
```

In order to pull out the links to the other pages, I will use the function
`dsst_wiki_get_links`. It requires passing an `xpath` argument. This one
selects links found in the "li" tag. There are a few common options for these
tags using Wikipedia data. I will help you choose the right one for your
specific task if you have not previously worked with xpath expressions.

```{r}
links <- dsst_wiki_get_links(obj, xpath = ".//li//a")
```

You can print out the links when working locally to determine if the code
worked correctly. Now, we finally use the `dsst_wiki_make_data` to grab the
pages for all of the links. This creates a `docs` table.

```{r}
docs <- dsst_wiki_make_data(links)
docs
```

The results of all these queries are being cached locally on your machine. The
first time it will take a while to run. Afterwards, it should progress much
faster.

When you are done, the `docs` table should look a lot like the tables that we
have previously worked with. There is no predictive task here, however, so no
`label` or `train_id` columns.

## Running the Annotations

Now, we need to create the `anno` table from the `docs` table. In the past I
have given this to you, but this time you will have to make it yourself. The
algorithm I used requires setting up Python, which is more trouble than it is
worth for one class project. Let's instead use a C-based algorithm that requires
no additional setup.  

Here is the code to run the annotations over the documents. We will also remove
any empty documents in the process, which can cause bugs later on.

```{r, eval=FALSE, message=FALSE, warning=FALSE}
library(cleanNLP)
cnlp_init_udpipe("english")

docs <- filter(docs, stringi::stri_length(text) > 0)
anno <- cnlp_annotate(docs)$token
```

The annotation process takes some time. Let's save the `docs` and `anno` table
for next time.

```{r, eval=FALSE}
write_csv(docs, file.path("..", "data", "wiki_list_of_fr_philosophers.csv"))
write_csv(anno, file.path("..", "data", "wiki_list_of_fr_philosophers_anno.csv.gz"))
```

The saved results can then be read-in and used without having to re-create the
data set. We will use this data in several of our upcoming notebooks.

## Another Example: List of Sovereign States

Let's look at one other example that illustrates a common trouble when using
the above code. We will start with the "List_of_sovereign_states", which has
all the current, officially recognised, countries in the world. As before, I
will use the function `dsst_wiki_load` to download the page from the official
Wikipedia API:

```{r}
obj <- dsst_wiki_load("List_of_sovereign_states")
```

The difficulty here is that the links are contained inside a table object. On
its own, this would be fine for the function `dsst_wiki_get_links` by just
using a different value for the `xpath` argument (`.//table//a`). However, in
this case we only want to get those links in the first column. This is a bit
trickier but fairly common. To fix this issue, I wrote a different function
called `dsst_wiki_get_links_table` that allows you to grab a specific table
and a specific column. Here is the function in action:

```{r}
links <- dsst_wiki_get_links_table(obj, table_num = 1L, column_num = 1L)
```

You will find that the often the function grabs one or two rows you do not
want. We will remove these manually:

```{r}
links <- links[-1,]
```

Once we have the links, we can make the pages from all the links just as before:

```{r}
docs <- dsst_wiki_make_data(links)
docs
```

And then make the annotation and save the results:

```{r, eval=FALSE, message=FALSE, warning=FALSE}
library(cleanNLP)
cnlp_init_udpipe("english")
anno <- cnlp_annotate(docs)$token

write_csv(docs, file.path("..", "data", "wiki_list_of_sovereign_states.csv"))
write_csv(anno, file.path("..", "data", "wiki_list_of_sovereign_states_anno.csv.gz"))
```

We will use both of these datasets in several of our upcoming notebooks.

## Other Languages

You can even use my code to build a dataset from another language's Wikipedia.
For example, here are the list of the countries for the German Wikipedia:

```{r}
obj <- dsst_wiki_load("Liste_der_Staaten_der_Erde", lang = "de")
```

Now, as before, we can try to get the links from the table:

```{r}
links <- dsst_wiki_get_links_table(obj, table_num = 1L, column_num = 1L)
links <- links[-1,]
links
```

And we have the same data format, just now in German:

```{r}
docs <- dsst_wiki_make_data(links, lang = "de")
docs
```

To parse, we need a specific parser for German:

```{r, eval=FALSE, message=FALSE, warning=FALSE}
library(cleanNLP)
cnlp_init_udpipe("german")
anno <- cnlp_annotate(docs)$token

write_csv(docs, file.path("..", "data", "wiki_list_of_sovereign_states_de.csv"))
write_csv(anno, file.path("..", "data", "wiki_list_of_sovereign_states_anno_de.csv.gz"))
```

We will look (briefly, my German is terrible) at this data as an example next
class.
