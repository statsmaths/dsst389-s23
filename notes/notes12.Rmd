---
title: "12. APIs and Text"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")
options(dsst.traceit = FALSE)
```

## Timezone API

Let's start with a relatively simple API that tells the current time in
different time zones. This is probably not a good candidate for caching, but
let's use it anyway as an example. To start, we specify the URL using the
protocol, authority, path, and query parameters.

```{r}
url_str <- modify_url(
  "https://www.timeapi.io/api/Time/current/zone",
  query = list("timeZone" = "Europe/Amsterdam")
)
```

Next, we call the HTTP GET method using our cache wrapper function. The result
can be parsed as JSON using the `content` function and and appropriate type.

```{r}
res <- dsst_cache_get(url_str, cache_dir = "cache", force = FALSE)
obj <- content(res, type = "application/json")
```

The object `obj` is a list object, in this case a set of name/values pairs.

```{r}
obj
```

We can access any specific element using the dollar sign operator, just as we
do with the objects returned by functions such as `dsst_enet_build`.

```{r}
obj$minute
```

## CNN Lite

While API is usually used to describe access points designed specifically for
programs to access data, we can use the same ideas to scrape data from a 
website. Your browser can be thought of as a program that uses an API to access
data in the form HTML, CSS, and JavaScript. Take a moment to look at the
[CNN Lite website](https://lite.cnn.com/). We'll try to grab data from this
page from within R.

The "API" here is simple; it has no query parameters. The data that is returned
is in a markup language called HTML, so we change the type of data that is 
returned by the `content` function:

```{r}
url_str <- modify_url("https://lite.cnn.com/")
res <- dsst_cache_get(url_str, cache_dir = "cache", force = FALSE)
obj <- content(res, type = "text/html", encoding = "UTF-8")
```

The object returned is a special type of R class that handles XML/HTML data.

```{r}
obj
```

We'll cover more details over the next few classes about how to use XML/HTML
objects. Here we'll just dive into some examples. To start, I'll use the 
`xml_find_all` function to find links (the tag "a") that are inside list items
(the tag "li"). These return each of the 100 stories on the front page of the
CNN lite website.

```{r}
xml_find_all(obj, "..//li/a")
```

We can extract the links to these pages using the function `xml_attr` and 
grabbing the "href" tag.

```{r}
temp <- xml_find_all(obj, "..//li/a")
links <- xml_attr(temp, "href")
head(links)
```

Once we have the links, we can grab the actual content of a specific link. 
For example, here we grab the first page:

```{r}
url_str <- modify_url(paste0("https://lite.cnn.com/", links[1]))
res <- dsst_cache_get(url_str, cache_dir = "cache", force = FALSE)
obj <- content(res, type = "text/html", encoding = "UTF-8")
```

Once again, the page is an HTML document.

```{r}
obj
```

From this, we can get each paragraph in the document:

```{r}
xml_find_all(obj, "..//p[@class='paragraph--lite']")
```

And with a little bit of cleaning, we have the full text of the article in R:

```{r}
text <- xml_find_all(obj, "..//p[@class='paragraph--lite']")
text <- xml_text(text)
text <- stri_trim(text)
head(text)
```

## Iteration over CNN Lite

We can use a for loop to cycle over each of the links and store the text from
all 100 stories. Let's try to do that now!

```{r}
text_all <- rep("", length(links))
for (j in seq_along(links))
{
  url_str <- modify_url(paste0("https://lite.cnn.com/", links[j]))
  res <- dsst_cache_get(url_str, cache_dir = "cache", force = FALSE)
  obj <- content(res, type = "text/html", encoding = "UTF-8")
  
  text <- xml_find_all(obj, "..//p[@class='paragraph--lite']")
  text <- xml_text(text)
  text <- stri_trim(text)
  
  text_all[j] <- paste0(text, collapse = " ")
}
```

Now, we can create a dataset that looks a lot like the `docs` tables that we
have been working with all semester:

```{r}
docs <- tibble(
  doc_id = sprintf("doc%04d", seq_along(links)),
  train_id = "train",
  text = text_all
)
docs
```

## Annotation

Now, we need to create the `anno` table from the `docs` table. In the past I
have given this to you, but this time you will have to make it yourself. The
algorithm I used requires setting up Python, which is more trouble than it is
worth for one class project. Let's instead use a C-based algorithm that requires
no additional setup.  

Here is the code to run the annotations over the documents. We will also remove
any empty documents in the process, which can cause bugs later on.

```{r, message=FALSE, warning=FALSE}
library(cleanNLP)
cnlp_init_udpipe("english")

docs <- filter(docs, stringi::stri_length(text) > 0)
anno <- cnlp_annotate(docs)$token
```

The annotation process takes some time, but shouldn't be too bad with only 100
short documents.

## And then?

Now, we can use all of the functions we have had in class on the data. There is
no straightforward predictive task, but we can use any of the unsupervised
algorithms to study the data. For example, here are the words with the highest
G-scores associated with each news article:

```{r}
anno %>%
  filter(upos %in% c("NOUN", "VERB")) %>%
  dsst_metrics(docs, label_var = "doc_id") %>%
  filter(count > expected) %>%
  group_by(label) %>%
  slice_head(n = 6L) %>%
  summarize(terms = paste0(token, collapse = "; ")) %>%
  getElement("terms")
```

Can you get a sense of the article topics but just looking at the top words?
