---
title: "13. JSON and Iteration"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")
options(dsst.traceit = FALSE)
```

Today, we are going to run through a number of different examples of creating
data sets through API calls. For the most part, these will make queries that 
return JSON data and require some form of iteration to process them. I will
show you two in the notes and two others in the notebook.

If you have no prior programming experience, this may be a bit overwhelming,
but  that's okay. I will make sure you have all of the API interface code you
actually need for Project 4 already outlined for you.

## Wikipedia Page Views

Let start start by using an API provided by Wikipedia. We will use this more
in the project, but here we focus just on using the API to see how many times
a Wikipedia page has been viewed in the past month.

We will start with defining a base URL using the protocol, authority, path, and
query parameters.

```{r}
url_base <- modify_url(
  "https://en.wikipedia.org/w/api.php",
  query = list(action = "query", format = "json", prop = "pageviews")
)
```

To fetch a particular page, we need add an additional parameter called titles
that contains the page title you want to grab. We can then return and parse 
the HTTP response as JSON.

```{r}
url_str <- modify_url(url_base, query = list(titles = "ChatGPT"))
res <- dsst_cache_get(url_str, cache_dir = "cache", force = FALSE)
obj <- content(res, type = "application/json")
```

The shape of `obj` is fairly complex. We could walk through it using the names
function, dollar sign, and bracket operator. In RStudio, you can look at the
object easily by clicking on it in the Environment Panel. With some
experimentation, you can see that the core of the data we want is nested here:

```{r}
pviews <- obj$query$pages[[1]]$pageviews
pviews
```

We can turn this into a rectangular data set by using the functions found in the
slides.

```{r}
dt <- tibble(
  page = "ChatGPT",
  date = ymd(names(pviews)),
  views = map_int(pviews, ~ dsst_null_to_na(..1))
)
dt
```

We can then plot the data and see if there are any interesting patterns:

```{r}
dt %>%
  filter(!is.na(views)) %>%
  mutate(wt = if_else(wday(date, week_start = 1) > 5, "weekend", "weekday")) %>%
  ggplot(aes(date, views)) +
    geom_line() +
    geom_point(aes(color = wt), size = 2, show.legend = FALSE) +
    scale_x_date(
      date_breaks = "1 week",
      date_labels = "%m/%d",
      date_minor_breaks = "1 week"
    ) +
    scale_color_viridis_d(begin = 0.2, end = 0.7) +
    labs(x = "Views", y = "Date") +
    dsst_tufte()
```

### XKCD

As a second task for the notes, let's create a data set from the API for the
popular web comic XKCD. The API has a slightly different structure because the
information about the kind of data we want to grab is stored in the path rather
than the query parameters. To get the URL for the 10th comic from XKCD, for
example, we need this code:

```{r}
i <- 10
url_str <- modify_url(sprintf("https://xkcd.com/%d/info.0.json", i))
```

In the following code, create a data set for the first 25 webcomes that has the
year, month, day, safe title, and transcript of the comics. Notice that I am
using some new code to store the data more easily within each loop.

```{r}
n <- 25
output <- vector("list", length = n)

for (i in seq_len(n))
{
  url_str <- modify_url(sprintf("https://xkcd.com/%d/info.0.json", i))
  
  res <- dsst_cache_get(url_str, cache_dir = "cache")
  
  obj <- content(res, type = "application/json")
  output[[i]] <- tibble(
    year = obj$year,
    month = obj$month,
    day = obj$day,
    safe_title = obj$safe_title,
    transcript = obj$transcript
  )
}

output <- bind_rows(output)
output
```

Note that it should be trivial to adjust your code to get all of the comics as
long as you know the number of the most recent comic. We are stopping at 25 
to reduce our load on their servers and also to avoid a long run.

