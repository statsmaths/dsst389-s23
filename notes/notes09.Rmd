---
title: "09. Unsupervised Learning II"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
options(dsst.traceit = FALSE)
```

## Load the Data

For today's notes we will work with one of the data sets from Project 2,
specifically the reviews of Music CDs.

```{r, message=FALSE}
docs <- read_csv("../data/amazon_cds.csv.bz2")
anno <- read_csv("../data/amazon_cds_token.csv.bz2")
```

Today, we will continue are study of unsupervised learning by studying a few
extensions of what we saw in the previous notes.

## Clusters with K-means

One thing that we often do when looking at plots of dimensionality reduction
is to look for clumps of documents that co-occur. We can do this explicit
by clustering the data using a clustering algorithm. Here, we will use a popular
option called K-means (note, this is not the same as k-NN, but I suppose it is
not entirely unrelated). It is an iterative algorithm that works as follows:

1. Pick the number of clusters N that you want to detect.
2. Randomly choose N data points as the starting centroids of each cluster.
3. Compute the distance of every data point to the centroids of the current
clusters.
4. Assign each data point to the cluster whose centroids it is closest.
5. Re-compute the cluster centroids as the average value of all the points in a
cluster.
6. Take the new cluster centroids, and repeat the process (compute distances,
reassign to groups, and recompute the centroids) iteratively until convergence.

The algorithm is not entirely deterministic because of the random starting
points. Typically, the algorithm is run several times and the "best" clustering
is chosen. How do we define the "best" in the case of a clustering algorithm?
A typical method is to measure the sum of squared distances to the cluster
centroids, a quantity that a good clustering will minimise.

- [visualisation of kmeans](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

Usually, K-means is run on a set of PCA coordinates rather than the entire
embedding itself. Often you will find that including a few dozen PCA components
provides a better fit (though it is rarely useful to use the entire TF-IDF).

To apply K-means, we will use the `dsst_kmeans` function. We need to set the
number of clusters; it is also possible to change the number of PCA dimensions.

```{r}
anno %>%
  filter(upos %in% c("NOUN", "ADJ", "VERB")) %>%
  inner_join(select(docs, -text), by = "doc_id") %>%
  dsst_pca(doc_var = "label", n_dims = 2) %>%
  dsst_kmeans(n_clusters = 5L)
```

Notice that these closely compare to the clusters you would see by grouping
nearby points in the PCA plot from last time.

This allows us to use the data to build other visualizations:

```{r}
anno %>%
  filter(upos %in% c("NOUN", "ADJ", "VERB")) %>%
  inner_join(select(docs, -text), by = "doc_id") %>%
  dsst_pca(doc_var = "label", n_dims = 2) %>%
  dsst_kmeans(n_clusters = 5) %>%
  ggplot(aes(v1, v2, color = factor(cluster))) +
    geom_point() +
    geom_text_repel(aes(label = label), show.legend = FALSE) +
    theme_void()
```

## Hierarchical Clustering

Another method for clustering analysis is to use a hierarchical approach. We
start by computing the nearest neighbor to each data point. Then, we combine
the two points that are closest together into a single cluster. Next, we
recompute the nearest neighbors using the center of the two combined points
in place of the individual points. This continues until all of the data are
in a single cluster.

To run hierarchical clustering, we can use the `dsst_hclust` function:

```{r}
model <- anno %>%
  filter(upos %in% c("NOUN", "ADJ", "VERB")) %>%
  inner_join(select(docs, -text), by = "doc_id") %>%
  dsst_pca(doc_var = "label", n_dims = 2) %>%
  dsst_hclust()
```

The hierarchy of clusters can be seen by plotting the model:

```{r}
plot(model)
```

We can create a concrete set of clusters by picking the number of clusters or
the height of the tree to cut at:

```{r}
dsst_hclust_cut(model, nclust = 10)
```

Hierarchical clustering works well for a small number of documents but can
become difficult to use when working with a larger set of documents.

## Word Relationships

In the preceding analyses, we have focused on the analysis of the
document their usage of words. It turns out that it is also possible to apply
dimensional reduction and distance metrics by swapping the words and the
documents. All of the concepts work the same, but now we have term frequencies
tell us how often each word is used in every document.

I included an easy-to-use argument to the `dsst_pca` function to compute the
PCA fit of this transposed analysis (as well as UMAP):

```{r, warning=FALSE}
dsst_pca(anno, invert = TRUE)
```

As well, we can compute clusters of words:

```{r, warning=FALSE}
anno %>%
  filter(upos %in% c("NOUN")) %>%
  dsst_pca(n_dims = 15L, invert = TRUE) %>%
  dsst_kmeans(n_clusters = 5L) %>%
  group_by(cluster) %>%
  summarize(words = paste(doc_id, collapse = "; "))
```

Do you see any other interesting patterns here?

Hopefully you are starting to see the possibilities of things that we can do
with this kind of method. For example, let's cluster the nouns, verbs, and
adjectives:

```{r, warning=FALSE}
df <- anno %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ")) %>%
  mutate(lemma = stri_paste(lemma, upos, sep = "_")) %>%
  dsst_pca(n_dims = 15L, invert = TRUE) %>%
  dsst_kmeans(n_clusters = 5L)
df
```

And then plot them according to the part of speech:

```{r, warning=FALSE}
df %>%
  mutate(upos = stri_extract(doc_id, regex = "[^_][A-Z]+\\Z")) %>%
  mutate(cluster = factor(cluster)) %>%
  ggplot(aes(v1, v2, color = upos)) +
    geom_text(aes(label = doc_id), show.legend = FALSE) +
    theme_void()
```
