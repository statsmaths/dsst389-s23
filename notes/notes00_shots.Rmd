---
title: "Bonus: Predicting NBA Shots"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
options(dsst.traceit = FALSE)

library(broom)
```

Today, let's briefly look at another kind of data and see how the techniques
we have been working with could be applied to it. Specifically, can we predict
whether an NBA player will make a shot:

```{r}
nba <- read_csv("../data/nba_shots.csv")
nba
```

As with most data sets that you will find outside of this class, there is no
prepopulated `train_id`. We can add one with the following code, which ensures
that the classes missed (0) and made (1) are evenly distributed.

```{r}
set.seed(1L)

nba <- nba %>%
  group_by(fgm) %>%
  mutate(rnum = runif(n())) %>%
  mutate(train_id = if_else(rnum > quantile(rnum, 0.6), "train", "valid")) %>%
  select(-rnum) %>%
  ungroup()
```

Since this data includes predefined features, we could just select a few and
build an unpenalized logistic regression model from these. This is nice because
we can immediately look at the coefficients and interpret them.

```{r}
model <- glm(
  fgm ~ shot_dist + shooter_height, data = nba,
  family = binomial(),
  subset = (train_id == "train")
)
summary(model)
```

Let's evaluate the model's error rate just as we do with our text models:

```{r}
nba$pred <- predict(model, nba, type = "response")

nba %>%
  mutate(pred_label = if_else(pred > 0.5, 1, 0)) %>%
  group_by(train_id) %>%
  summarize(erate = mean(pred_label != fgm))
```

Next, we can try to fit a penalized model on all of the variables in the data.
To do this, we need a few fancy functions from base R to turn the data into a
numeric matrix (the real work is only needed to turn the player names into 
indicator variables; otherwise we could just use `as.matrix`):

```{r}
mf <- model.frame(fgm ~ -1 + ., data = select(nba, -train_id, -pred))
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)
```

Notice that because of the indicator variables, the size of the model matrix is
much larger than the number of variables in the original data:

```{r}
dim(X)
```


Now, split out the training data:

```{r}
X_train <- X[nba$train_id == "train", ]
y_train <- y[nba$train_id == "train"]
```

And then apply the penalized logistic regression function:

```{r}
library(glmnet)

model <- cv.glmnet(X_train, y_train, family = "binomial", nfolds = 3L)
nba$pred <- as.numeric(predict(model, X, type = "response"))
```

As with the textual data, we can look at the coefficients:

```{r}
beta <- coef(model)
beta[as.numeric(beta) != 0,,drop=FALSE]
```

And the error rate of the model, which is only slightly better than the model
using just two variables:

```{r}
nba %>%
  mutate(pred_label = if_else(pred > 0.5, 1, 0)) %>%
  group_by(train_id) %>%
  summarize(erate = mean(pred_label != fgm))
```

Finally, let's fit a boosted trees model on the data. This requires just a
little more setup:

```{r, message=FALSE}
library(xgboost)
X_valid <- X[nba$train_id == "valid", ]
y_valid <- y[nba$train_id == "valid"]

data_train <- xgb.DMatrix(data = X_train, label = y_train)
data_valid <- xgb.DMatrix(data = X_valid, label = y_valid)
watchlist <- list(train=data_train, valid=data_valid)
```

And then we'll run it over 400 trees with a small learning rate (0.01):

```{r}
model <- xgb.train(data = data_train,
                   max_depth = 5,
                   eta = 0.01,
                   nthread = 2,
                   nrounds = 400,
                   objective = "multi:softmax",
                   eval_metric = "mlogloss",
                   watchlist = watchlist,
                   verbose = TRUE,
                   print_every_n = 25,
                   num_class = 2L)
```

The results are, again, only slightly better than the other models:

```{r}
nba$pred <- predict(model, newdata = X)

nba %>%
  mutate(pred_label = if_else(pred > 0.5, 1, 0)) %>%
  group_by(train_id) %>%
  summarize(erate = mean(pred_label != fgm))
```

Hopefully this helps illustrate how we use the techniques from this class with
other data types. It also hopefully illustrates why working with textual data,
where the features are not pre-determined, makes for a more interesting study
in this class than other data would. Structured data is, of course, very 
interesting, but much of this is lost unless you are working with data from a
domain in which you are knowledgeable and have concrete research questions for.
