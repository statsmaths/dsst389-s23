---
title: "Notebook 14"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

## Exploring Richmond, Virginia Page

Start by constructing the URL to grab the text for the Wikipedia page named
"Richmond,_Virginia" from the MediaWiki API.

```{r, question-01}

```

Now, make the API request and store the output as an R object called `obj`.

```{r, question-02}

```

Next, parse the XML/HTML text using the function `read_html`.

```{r, question-03}

```

Use the `xml_find_all` to find all of the nodes with the tag "h2". Look at the
Wikipedia page in a browser and try to find what these correspond to.

```{r, question-04}

```

Try to use a new `xpath` argument below to extract just the name of each 
section. Turn it into an R string object using `xml_text`:

```{r, question-05}

```

Repeat the previous question for the tag "h3". What are these?

```{r, question-06}

```

Now, extract the paragraphs using the "p" tag.

```{r, question-07}

```

Continue by using `xml_text` to extract the text from each paragraph. Take a 
few minutes to look through the results to see why we need some special logic
to turn the output into something we can parse as text.

```{r, question-08}

```

Now, create a tibble object with one column called `links` and only one row 
with the entry "Richmond,_Virginia" for the variable `links`. This is the format
required in the next code block. Make sure to save the result.

```{r, question-10}

```

Pass your data from the last code to the function `dsst_wiki_make_data`. Use
the code from the notes to post-process the results and look through the text
a bit to see how it corresponds with the result from your own usage of 
`xml_text`.

```{r, question-11}

```

## Building a Corpus of Virginia Cities

Let's build a large corpus. Start by constructing the url for the page
called "List_of_towns_in_Virginia".

```{r, question-12}

```

Now, call the API to grab the results and create an object `tree` that contains
the parsed text of the page.

```{r, question-13}

```

Try to use the function `dsst_wiki_get_links_table` to get links to each of
the cities. This will take a bit of trial and error and looking at the actual
Wikipedia page.

```{r, question-14}

```

Once you have the data, use `dsst_wiki_make_data` and the code from the notes
to contruct a full `docs` table for the cities.

```{r, question-15}

```

Now, parse the text using the `cnlp_annotate` function:

```{r, question-16}

```

And finally, display the top 5 NOUNS/VERBS from each town's page (Hint: you 
should be able to copy the code from the end of Notebook13).

```{r, question-17}

```

