---
title: "Notebook 12"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

## Getting Started

Before running this notebook, select "Session > Restart R and Clear Output" in
the menu above to start a new R session. This will clear any old data sets and
give us a blank slate to start with.

After starting a new session, run the following code chunk to load the
libraries and data that we will be working with today.

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

## Time API

Create a URL string to find the current time in the "America/Chicago" timezone.

```{r, question-01}

```

Run the query and print out the returned object. Verify that the time is one
hour behind the current time in Richmond, VA.

```{r, question-02}

```

The time API has another access point that gives the current time based on a
latitude and longitude. The query starts with the URL
"https://www.timeapi.io/api/Time/current/zone" and requires two query parameters,
loatitude and longitude. Create a query string to detect the time in the city of
Victoria, Seychelles. You'll need to look up its latitude and longitude online.

```{r, question-03}

```

Run the query and print out the results. How far ahead is the time in the
Seychelles from the time in Virginia?

```{r, question-04}

```

## CNN Lite

Let's now return to the CNN Lite dataset. You'll have different results from 
the notes because you are creating the dataset on a different day. Note that
while you should be able to copy much of the code directly from the notes (and
that's okay!), try to only copy the code that is actually needed rather than all
the intermediate steps I only showed to illustrate the method.

To start, grab the HTML data from the front page of the CNN Lite website.

```{r, question-05}

```

Extract the links to each story and save them as an object called `links`.

```{r, question-06}

```

Use the code below to create short titles for each of the documents. We will 
use these later as document ids.

```{r}
title <- xml_text(xml_find_all(obj, "..//li/a"))
title <- stri_trim(title)
title <- stri_sub(title, 1L, 40L)
head(title)
```

Now, iterate over the stories and create a vector `text_all` that has one
element for each story that contains the entire story's text.

```{r, question-07}

```

Build a `docs` table, but unlike in the notes use the short title of the
document as the `doc_id`. Print out the table and verify that it matches the
current version of the website.

```{r, question-08}

```

Run the annotation algorithm over the documents. Print out the `anno` table and
verify that it has the expected structure that matches what we have been using
all semester.

```{r, question-09}

```

Now, with the textual data, create a PCA plot of the 100 news articles using 
just nouns and verbs:

```{r, question-10}

```

Let's add some context to the above plot. The titles would be too long to read.
instead, create a data table called `tterms` that contains one noun or verb with
the highest G-score that is associated with each document.

```{r, question-11}

```

Start by repeating the plot in question 10, but join to the `tterms` data and
add a text repel layer showing the strongest associated word with each document.
You might want to set the size of the labels to something smaller (2?) to make
the plot easier to read. Can you start to explain the shape of the PCA plot?

```{r, question-12}

```

Repeat the previous question but using UMAP instead of PCA. Note that the plot
functions very differently (at least with the stories I had the day I wrote
this, the UMAP was more interesting).

```{r, question-13}

```

Finally, let's build a topic model using 16 topics and only nouns, verbs,
adjectives, and adverbs. Save the result as an object called `model`.

```{r, question-14}

```

Export the results as a JSON file and explore using the topic model
visualization tool here: https://statsmaths.github.io/topic-explo/build/

```{r, question-15}

```

